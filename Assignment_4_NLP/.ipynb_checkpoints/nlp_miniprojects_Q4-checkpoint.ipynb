{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Importing modules'''\n",
    "import pandas\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, coo_matrix\n",
    "import json\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin, TransformerMixin\n",
    "from sklearn.linear_model import Ridge, LinearRegression, Lasso, SGDRegressor\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pickle\n",
    "import dill\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from nltk.corpus import stopwords\n",
    "from sys import getsizeof\n",
    "import re\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "'''Initializing variables and empty lists'''\n",
    "nlp_dict_list = []\n",
    "ml_dict_list = []\n",
    "nlp_iter_lines = 0\n",
    "ml_iter_lines = 0\n",
    "iter_lower_split = 0\n",
    "text_list_lower_split = []\n",
    "business_id_list = []\n",
    "a = 0\n",
    "b = 0\n",
    "text_list = []\n",
    "\n",
    "'''Opening txt file containing Yelp review data and reading all lines'''\n",
    "nlp_yelp_dataset_txt = open('yelp_train_academic_dataset_review.txt', 'rb+')\n",
    "nlp_raw_lines = nlp_yelp_dataset_txt.readlines()\n",
    "ml_yelp_dataset_txt = open('../Assignment_3_ML/yelp_dataset_Raw.txt', 'rb+')\n",
    "ml_raw_lines = ml_yelp_dataset_txt.readlines()\n",
    "\n",
    "'''Writing nlp raw_lines to an empty list using json.loads to parse'''\n",
    "while nlp_iter_lines < 1000: #len(nlp_raw_lines):\n",
    "    if len(nlp_raw_lines[nlp_iter_lines]) > 0:\n",
    "        nlp_dict_list.append(json.loads(nlp_raw_lines[nlp_iter_lines]))\n",
    "    nlp_iter_lines += 1\n",
    "    \n",
    "'''Writing ml raw_lines to an empty list using json.loads to parse'''\n",
    "while ml_iter_lines < 1000: #len(ml_raw_lines):\n",
    "    if len(ml_raw_lines[ml_iter_lines]) > 0:\n",
    "        ml_dict_list.append(json.loads(ml_raw_lines[ml_iter_lines]))\n",
    "    ml_iter_lines += 1\n",
    "    \n",
    "'''Initializing list with Yelp data into a pandas data frame for manipulation'''\n",
    "nlp_data_frame = pandas.DataFrame(nlp_dict_list).to_dict()\n",
    "ml_data_frame = pandas.DataFrame(ml_dict_list).to_dict()\n",
    "\n",
    "'''Closing txt files'''\n",
    "nlp_yelp_dataset_txt.close()\n",
    "ml_yelp_dataset_txt.close()\n",
    "\n",
    "'''Initializing stars and text lists'''\n",
    "nlp_stars_list = nlp_data_frame['stars'].values()\n",
    "nlp_text_list = nlp_data_frame['text'].values()\n",
    "nlp_business_id = nlp_data_frame['business_id'].values()\n",
    "ml_business_id = ml_data_frame['business_id'].values()\n",
    "ml_category = ml_data_frame['categories'].values()\n",
    "\n",
    "'''Checking and matching the business id to restaurant categories'''\n",
    "for index, categories in enumerate(ml_category):\n",
    "    for inner_index, inner_categories in enumerate(categories):\n",
    "        if inner_categories == 'Restaurants':\n",
    "            business_id_list.append(ml_business_id[index])\n",
    "\n",
    "while b < len(nlp_business_id):\n",
    "    if [business_id for business_id in business_id_list if business_id in nlp_business_id[b]]:\n",
    "        text_list.append(nlp_text_list[b])\n",
    "    b += 1\n",
    "\n",
    "'''Initializing stopwords and adding new stopwords to the list'''\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stop_words_additions = ['ll', 'adwmuxsza', 'zu', 'abc', 'aac', 'aardbark', 'aabc', 'aab', 'aaa', 'aa', 'that', 'youre', 'zzcrkebcrfxbb', 'zse', 'i\\'m', 'he\\'s', 'i\\'ve', 'it\\'s', 'id', 'im', 'hes', 'ive', 'its']\n",
    "while a < len(stop_words_additions):\n",
    "    stop_words.append(stop_words_additions[a])\n",
    "    a += 1\n",
    "    \n",
    "'''Looping through text_list converting to lower case and splitting into individual terms'''\n",
    "while iter_lower_split < len(text_list):\n",
    "    text_list_lower = text_list[iter_lower_split].lower()\n",
    "    text_list_split = text_list_lower.split()\n",
    "    text_list_split = [w for w in text_list_split if not w in stop_words]\n",
    "    text_list_split = (' '.join(text_list_split))\n",
    "    text_list_split = re.findall('[a-z]{2,}', text_list_split)\n",
    "    text_list_split = (' '.join(text_list_split))\n",
    "    text_list_lower_split.append(text_list_split)\n",
    "    iter_lower_split += 1\n",
    "    \n",
    "'''Initializing analytical model axes'''\n",
    "X = text_list_lower_split\n",
    "y = nlp_stars_list\n",
    "    \n",
    "count_vectorizer_unigram = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None)\n",
    "count_vectorizer_bigram = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None, ngram_range = (2, 2))\n",
    "\n",
    "cv_unigram = count_vectorizer_unigram.fit_transform(X, y)\n",
    "cv_unigram = np.asarray(cv_unigram.toarray())\n",
    "\n",
    "cv_bigram = count_vectorizer_bigram.fit_transform(X, y)\n",
    "cv_bigram = np.asarray(cv_bigram.toarray())\n",
    "\n",
    "unigram_count = zip(count_vectorizer_unigram.get_feature_names(), cv_unigram.sum(axis = 0))\n",
    "bigram_count = zip(count_vectorizer_bigram.get_feature_names(), cv_bigram.sum(axis = 0))\n",
    "\n",
    "unigram_dict = {}\n",
    "bigram_dict = {}\n",
    "\n",
    "for unigram in unigram_count:\n",
    "    unigram_dict.update({unigram[0]: unigram[1]})\n",
    "    \n",
    "for bigram in bigram_count:\n",
    "    bigram_dict.update({bigram[0]: bigram[1]})\n",
    "\n",
    "for bigrams in bigram_dict:\n",
    "    bigrams_word  = bigrams.split()\n",
    "    bigrams_count = bigram_dict[bigrams]\n",
    "    score = (bigrams_count * 1.0) / (unigram_dict[bigrams_word[0]] * unigram_dict[bigrams_word[1]] + 15000)\n",
    "    print str(bigrams) + '   -   ' + str(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
